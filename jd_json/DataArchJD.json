{
  "Job_Title": "Sr. Data Architect",
  "Required_Skills": [
    "Databricks",
    "Apache Spark",
    "SQL",
    "Python",
    "Scala",
    "Azure",
    "AWS",
    "GCP"
  ],
  "Preferred_Skills": [
    "LangChain",
    "AutoGen",
    "CrewAI",
    "SageMaker",
    "Vertex AI",
    "Azure ML",
    "Kafka",
    "Spark Streaming"
  ],
  "Minimum_Experience": "10+ years",
  "Location": "San Francisco Bay Area",
  "Responsibilities": [
    "Design and implement high-performance data architectures leveraging Databricks, Apache Spark, and Delta Lake.",
    "Develop optimized ETL/ELT workflows using Databricks Workflows, Delta Live Tables.",
    "Design data lakehouse architectures in Azure/AWS/GCP.",
    "Optimize Spark jobs, query performance, and cluster configurations.",
    "Implement data quality frameworks, access controls, and compliance measures.",
    "Work closely with engineering teams, data scientists, and business analysts."
  ],
  "Qualifications": [
    "Experience in Data Architecture",
    "Proficiency in SQL, Python, Scala",
    "Deep understanding of Delta Lake, Medallion Architecture",
    "Knowledge of distributed computing, data pipeline orchestration"
  ],
  "Domain": "Data Architecture"
}